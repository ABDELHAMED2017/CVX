
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>Figure 7.1: Logistic regression (GP version)</title>
      <meta name="generator" content="MATLAB 7.4">
      <meta name="date" content="2007-08-10">
      <meta name="m-file" content="logistics_gp"><style>

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head>
   <body>
      <div class="content">
         <h1>Figure 7.1: Logistic regression (GP version)</h1><pre class="codeinput"><span class="comment">% Section 7.1.1</span>
<span class="comment">% Boyd &amp; Vandenberghe, "Convex Optimization"</span>
<span class="comment">% Kim &amp; Mutapcic, "Logistic regression via geometric programming"</span>
<span class="comment">% Written for CVX by Almir Mutapcic 02/08/06</span>
<span class="comment">%</span>
<span class="comment">% Solves the logistic regression problem re-formulated as a GP.</span>
<span class="comment">% The original log regression problem is:</span>
<span class="comment">%</span>
<span class="comment">%   minimize   sum_i(theta'*x_i) + sum_i( log(1 + exp(-theta'*x_i)) )</span>
<span class="comment">%</span>
<span class="comment">% where x are explanatory variables and theta are model parameters.</span>
<span class="comment">% The equivalent GP is obtained by the following change of variables:</span>
<span class="comment">% z_i = exp(theta_i). The log regression problem is then a GP:</span>
<span class="comment">%</span>
<span class="comment">%   minimize   prod( prod(z_j^x_j) ) * (prod( 1 + prod(z_j^(-x_j)) ))</span>
<span class="comment">%</span>
<span class="comment">% with variables z and data x (explanatory variables).</span>

randn(<span class="string">'state'</span>,0);
rand(<span class="string">'state'</span>,0);

a =  1;
b = -5;

m = 100;
u = 10*rand(m,1);
y = (rand(m,1) &lt; exp(a*u+b)./(1+exp(a*u+b)));

<span class="comment">% order the observation data</span>
ind_false = find( y == 0 );
ind_true  = find( y == 1 );

<span class="comment">% X is the sorted design matrix</span>
<span class="comment">% first have true than false observations followed by the bias term</span>
X = [u(ind_true); u(ind_false)];
X = [X ones(size(u,1),1)];
[m,n] = size(X);
q = length(ind_true);

cvx_begin <span class="string">gp</span>
  <span class="comment">% optimization variables</span>
  variables <span class="string">z(n)</span> <span class="string">t(q)</span> <span class="string">s(m)</span>

  minimize( prod(t)*prod(s) )
  subject <span class="string">to</span>
    <span class="keyword">for</span> k = 1:q
      prod( z.^(X(k,:)') ) &lt;= t(k);
    <span class="keyword">end</span>

    <span class="keyword">for</span> k = 1:m
      1 + prod( z.^(-X(k,:)') ) &lt;= s(k);
    <span class="keyword">end</span>
cvx_end

<span class="comment">% retrieve the optimal values and plot the result</span>
theta = log(z);
aml = -theta(1);
bml = -theta(2);

us = linspace(-1,11,1000)';
ps = exp(aml*us + bml)./(1+exp(aml*us+bml));

plot(us,ps,<span class="string">'-'</span>, u(ind_true),y(ind_true),<span class="string">'o'</span>, <span class="keyword">...</span>
                u(ind_false),y(ind_false),<span class="string">'o'</span>);
axis([-1, 11,-0.1,1.1]);
</pre><pre class="codeoutput"> 
Calling SDPT3: 1953 variables, 955 equality constraints
Note: for improved efficiency, SDPT3 is solving the dual problem.
------------------------------------------------------------

 num. of constraints = 955
 dim. of sdp    var  = 500,   num. of sdp  blk  = 100
 dim. of socp   var  = 200,   num. of socp blk  = 100
 dim. of linear var  = 253
*******************************************************************
   SDPT3: Infeasible path-following algorithms
*******************************************************************
 version  predcorr  gam  expon  scale_data
    NT      1      0.000   1        0    
it pstep dstep pinfeas dinfeas  gap      mean(obj)   cputime
-------------------------------------------------------------------
 0|0.000|0.000|3.8e+02|1.6e+01|1.9e+07| 3.834269e+05| 0:0:00| spchol  1  1 
 1|0.406|0.267|2.3e+02|1.2e+01|1.5e+07| 4.128586e+05| 0:0:00| spchol  1  1 
 2|0.424|0.644|1.3e+02|4.2e+00|9.4e+06| 3.990932e+05| 0:0:00| spchol  1  1 
 3|0.933|1.000|8.8e+00|8.8e-02|9.1e+05| 1.943383e+05| 0:0:00| spchol  1  1 
 4|0.923|1.000|6.7e-01|4.4e-02|7.9e+04| 1.447306e+04| 0:0:01| spchol  1  1 
 5|0.760|1.000|1.6e-01|1.3e-02|2.9e+04| 5.500940e+03| 0:0:01| spchol  1  1 
 6|0.973|0.858|4.3e-03|5.3e-03|2.1e+03|-5.123495e+02| 0:0:01| spchol  1  1 
 7|0.947|0.983|2.3e-04|1.3e-03|2.5e+02|-4.280058e+01| 0:0:01| spchol  1  1 
 8|0.904|0.924|2.2e-05|1.8e-04|3.5e+01|-2.372168e+01| 0:0:01| spchol  1  1 
 9|1.000|0.955|3.5e-10|1.7e-05|1.2e+01|-2.341175e+01| 0:0:01| spchol  1  1 
10|0.906|0.911|3.3e-11|1.8e-06|1.2e+00|-2.144507e+01| 0:0:01| spchol  1  1 
11|1.000|1.000|1.5e-13|4.0e-08|5.0e-01|-2.130612e+01| 0:0:01| spchol  1  1 
12|0.922|0.920|6.9e-14|6.8e-09|4.7e-02|-2.121788e+01| 0:0:02| spchol  1  1 
13|1.000|1.000|6.4e-11|4.0e-10|1.3e-02|-2.121100e+01| 0:0:02| spchol  1  1 
14|0.957|0.958|3.4e-11|5.6e-11|6.2e-04|-2.120835e+01| 0:0:02| spchol  1  1 
15|1.000|1.000|4.3e-12|2.3e-12|4.0e-05|-2.120823e+01| 0:0:02| spchol  1  1 
16|0.999|0.999|6.8e-13|1.0e-12|4.5e-07|-2.120822e+01| 0:0:02|
  sqlp stop: max(relative gap, infeasibilities) &lt; 1.49e-08
-------------------------------------------------------------------
 number of iterations   = 16
 primal objective value = -2.12082247e+01
 dual   objective value = -2.12082251e+01
 gap := trace(XZ)       = 4.46e-07
 relative gap           = 1.03e-08
 actual relative gap    = 1.03e-08
 rel. primal infeas     = 6.76e-13
 rel. dual   infeas     = 1.00e-12
 norm(X), norm(y), norm(Z) = 1.6e+01, 1.7e+01, 3.7e+01
 norm(A), norm(b), norm(C) = 6.2e+01, 1.2e+02, 4.1e+01
 Total CPU time (secs)  = 2.2  
 CPU time per iteration = 0.1  
 termination code       =  0
 DIMACS: 6.8e-13  0.0e+00  8.9e-12  0.0e+00  1.0e-08  1.0e-08
-------------------------------------------------------------------
------------------------------------------------------------
Status: Solved
Optimal value (cvx_optval): +2.18281e+14
</pre><img vspace="5" hspace="5" src="logistics_gp_01.png"> <p class="footer"><br>
            Published with MATLAB&reg; 7.4<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
%% Figure 7.1: Logistic regression (GP version)

% Section 7.1.1
% Boyd & Vandenberghe, "Convex Optimization"
% Kim & Mutapcic, "Logistic regression via geometric programming"
% Written for CVX by Almir Mutapcic 02/08/06
%
% Solves the logistic regression problem re-formulated as a GP.
% The original log regression problem is:
%
%   minimize   sum_i(theta'*x_i) + sum_i( log(1 + exp(-theta'*x_i)) )
%
% where x are explanatory variables and theta are model parameters.
% The equivalent GP is obtained by the following change of variables:
% z_i = exp(theta_i). The log regression problem is then a GP:
%
%   minimize   prod( prod(z_j^x_j) ) * (prod( 1 + prod(z_j^(-x_j)) ))
%
% with variables z and data x (explanatory variables).

randn('state',0);
rand('state',0);

a =  1;
b = -5;

m = 100;
u = 10*rand(m,1);
y = (rand(m,1) < exp(a*u+b)./(1+exp(a*u+b)));

% order the observation data
ind_false = find( y == 0 );
ind_true  = find( y == 1 );

% X is the sorted design matrix
% first have true than false observations followed by the bias term
X = [u(ind_true); u(ind_false)];
X = [X ones(size(u,1),1)];
[m,n] = size(X);
q = length(ind_true);

cvx_begin gp
  % optimization variables
  variables z(n) t(q) s(m)

  minimize( prod(t)*prod(s) )
  subject to
    for k = 1:q
      prod( z.^(X(k,:)') ) <= t(k);
    end

    for k = 1:m
      1 + prod( z.^(-X(k,:)') ) <= s(k);
    end
cvx_end

% retrieve the optimal values and plot the result
theta = log(z);
aml = -theta(1);
bml = -theta(2);

us = linspace(-1,11,1000)';
ps = exp(aml*us + bml)./(1+exp(aml*us+bml));

plot(us,ps,'-', u(ind_true),y(ind_true),'o', ...
                u(ind_false),y(ind_false),'o');
axis([-1, 11,-0.1,1.1]);

##### SOURCE END #####
-->
   </body>
</html>